{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers --quiet\n",
    "!pip install seaborn --quiet\n",
    "!pip install sklearn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Sevann\\UBA\\procesamiento_lenguaje_natural\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Sevann\\UBA\\procesamiento_lenguaje_natural\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load and Prepare the BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert_model(trainable=False):\n",
    "    \"\"\"\n",
    "    Load the pre-trained BERT model with the option to freeze or unfreeze its layers.\n",
    "    \"\"\"\n",
    "    model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    model.bert.trainable = trainable\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Build the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classification_model(num_classes, bert_model, max_length=140, learning_rate=2e-5):\n",
    "    \"\"\"\n",
    "    Build a classification model with BERT as the base and a Dense layer for classification.\n",
    "    \"\"\"\n",
    "    # Input layers\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
    "    \n",
    "    # BERT output\n",
    "    outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = outputs.pooler_output\n",
    "    \n",
    "    # Dropout and Dense layer for classification\n",
    "    x = tf.keras.layers.Dropout(0.2)(pooled_output)\n",
    "    output_layer = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Build and compile the model\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output_layer)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),  \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def preprocess_data(df, target_col, num_classes):\n",
    "    y = tf.keras.utils.to_categorical(df[target_col].values, num_classes=num_classes)\n",
    "    sentences = df['content'].values    \n",
    "    return sentences, y\n",
    "\n",
    "def tokenize_data(tokenizer, sentences, max_length=140):\n",
    "    tokens = tokenizer(sentences, max_length=max_length, padding=True, truncation=True, return_tensors='tf')\n",
    "    return tokens['input_ids'], tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_val, y_val, epochs=5):\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), batch_size=32)\n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, class_names):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excecution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Sevann\\UBA\\procesamiento_lenguaje_natural\\venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model = load_bert_model(trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Process data for 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_3_classes'] = df.score.apply(lambda x: 0 if x <= 2 else 1 if x == 3 else 2)\n",
    "class_names_3 = ['negative', 'neutral', 'positive']\n",
    "sentences_3, y_3_classes = preprocess_data(df, 'sentiment_3_classes', num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Split and tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Sevann\\UBA\\procesamiento_lenguaje_natural\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "X_train_val, X_test, y_train_val, y_test_3_classes = train_test_split(sentences_3, y_3_classes, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.tolist()\n",
    "X_val = X_val.tolist()\n",
    "X_test = X_test.tolist()\n",
    "\n",
    "X_train_tokens = tokenize_data(tokenizer, X_train)\n",
    "X_val_tokens = tokenize_data(tokenizer, X_val)\n",
    "X_test_tokens = tokenize_data(tokenizer, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Build and train the model for 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Sevann\\UBA\\procesamiento_lenguaje_natural\\venv\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Sevann\\UBA\\procesamiento_lenguaje_natural\\venv\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "315/315 [==============================] - 2264s 7s/step - loss: 1.1003 - accuracy: 0.4253 - val_loss: 0.9727 - val_accuracy: 0.5385\n",
      "Epoch 2/5\n",
      "315/315 [==============================] - 2203s 7s/step - loss: 1.0044 - accuracy: 0.4945 - val_loss: 0.9191 - val_accuracy: 0.5611\n",
      "Epoch 3/5\n",
      "315/315 [==============================] - 2572s 8s/step - loss: 0.9687 - accuracy: 0.5251 - val_loss: 0.9078 - val_accuracy: 0.5417\n",
      "Epoch 4/5\n",
      "315/315 [==============================] - 2183s 7s/step - loss: 0.9616 - accuracy: 0.5223 - val_loss: 0.9326 - val_accuracy: 0.5528\n",
      "Epoch 5/5\n",
      "315/315 [==============================] - 2563s 8s/step - loss: 0.9550 - accuracy: 0.5331 - val_loss: 0.8957 - val_accuracy: 0.5635\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-3\n",
    "model_3_classes = build_classification_model(num_classes=3, bert_model=bert_model, learning_rate=learning_rate)\n",
    "history_3_classes = train_and_evaluate_model(model_3_classes, [X_train_tokens[0], X_train_tokens[1]], y_train, [X_val_tokens[0], X_val_tokens[1]], y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate the model for 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 563s 6s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.81      0.64       997\n",
      "     neutral       0.53      0.08      0.14       979\n",
      "    positive       0.64      0.79      0.70      1174\n",
      "\n",
      "    accuracy                           0.58      3150\n",
      "   macro avg       0.56      0.56      0.49      3150\n",
      "weighted avg       0.57      0.58      0.51      3150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_3_classes, [X_test_tokens[0], X_test_tokens[1]], y_test_3_classes, class_names_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Process data for 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_5_classes'] = df.score.apply(lambda x: x - 1)\n",
    "class_names_5 = ['1', '2', '3', '4', '5']\n",
    "sentences_5, y_5_classes = preprocess_data(df, 'sentiment_5_classes', num_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Split and tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test_5_classes = train_test_split(sentences_5, y_5_classes, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.tolist()\n",
    "X_val = X_val.tolist()\n",
    "X_test = X_test.tolist()\n",
    "\n",
    "X_train_tokens = tokenize_data(tokenizer, X_train)\n",
    "X_val_tokens = tokenize_data(tokenizer, X_val)\n",
    "X_test_tokens = tokenize_data(tokenizer, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Build and train the model for 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "315/315 [==============================] - 2353s 7s/step - loss: 1.6767 - accuracy: 0.2785 - val_loss: 1.5873 - val_accuracy: 0.3175\n",
      "Epoch 2/5\n",
      "315/315 [==============================] - 1970s 6s/step - loss: 1.6399 - accuracy: 0.2770 - val_loss: 1.5722 - val_accuracy: 0.3179\n",
      "Epoch 3/5\n",
      "315/315 [==============================] - 1845s 6s/step - loss: 1.6281 - accuracy: 0.2799 - val_loss: 1.5661 - val_accuracy: 0.3179\n",
      "Epoch 4/5\n",
      "315/315 [==============================] - 1853s 6s/step - loss: 1.6238 - accuracy: 0.2850 - val_loss: 1.5591 - val_accuracy: 0.3179\n",
      "Epoch 5/5\n",
      "315/315 [==============================] - 1816s 6s/step - loss: 1.6242 - accuracy: 0.2778 - val_loss: 1.5563 - val_accuracy: 0.3183\n"
     ]
    }
   ],
   "source": [
    "model_5_classes = build_classification_model(num_classes=5, bert_model=bert_model)\n",
    "history_5_classes = train_and_evaluate_model(model_5_classes, [X_train_tokens[0], X_train_tokens[1]], y_train, [X_val_tokens[0], X_val_tokens[1]], y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Evaluate the model for 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 482s 5s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.00      0.01       515\n",
      "           2       0.00      0.00      0.00       482\n",
      "           3       0.31      0.99      0.48       979\n",
      "           4       0.17      0.00      0.00       591\n",
      "           5       0.24      0.01      0.01       583\n",
      "\n",
      "    accuracy                           0.31      3150\n",
      "   macro avg       0.24      0.20      0.10      3150\n",
      "weighted avg       0.25      0.31      0.15      3150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_5_classes, [X_test_tokens[0], X_test_tokens[1]], y_test_5_classes, class_names_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Sevann\\UBA\\procesamiento_lenguaje_natural\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar los datos\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "max_length = 140\n",
    "\n",
    "def tokenize(sentences):\n",
    "    return bert_tokenizer(sentences, max_length=max_length, padding=True, truncation=True, return_tensors='tf')\n",
    "\n",
    "def analyze_sentence(text, model_3_classes, model_5_classes, class_names_3, class_names_5):\n",
    "    # Tokenizar la frase\n",
    "    tokens = tokenize([text])\n",
    "    \n",
    "    # Predecir la clase para la frase con el modelo de 3 clases\n",
    "    prediction_3_classes = model_3_classes.predict([tokens['input_ids'], tokens['attention_mask']])\n",
    "    predicted_class_3 = np.argmax(prediction_3_classes, axis=1)[0]\n",
    "    \n",
    "    # Predecir la clase para la frase con el modelo de 5 clases\n",
    "    prediction_5_classes = model_5_classes.predict([tokens['input_ids'], tokens['attention_mask']])\n",
    "    predicted_class_5 = np.argmax(prediction_5_classes, axis=1)[0]\n",
    "    \n",
    "    # Obtener las etiquetas de las clases predichas\n",
    "    pred_class_3 = class_names_3[predicted_class_3]\n",
    "    pred_class_5 = class_names_5[predicted_class_5]\n",
    "    \n",
    "    print(f\"Frase: {text}\")\n",
    "    print(f\"Predicción (3 clases): {pred_class_3}\")\n",
    "    print(f\"Predicción (5 clases): {pred_class_5}\")\n",
    "    print()\n",
    "    \n",
    "    return pred_class_3, pred_class_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Frase: This app is amazing, I use it every day!\n",
      "Predicción (3 clases): positive\n",
      "Predicción (5 clases): 3\n",
      "\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Frase: Terrible experience, the app crashes all the time.\n",
      "Predicción (3 clases): negative\n",
      "Predicción (5 clases): 3\n",
      "\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Frase: It's a decent app, but it has some bugs.\n",
      "Predicción (3 clases): negative\n",
      "Predicción (5 clases): 3\n",
      "\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Frase: The app is okay, but there are better alternatives.\n",
      "Predicción (3 clases): negative\n",
      "Predicción (5 clases): 3\n",
      "\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Frase: I hate the new update, it ruined everything.\n",
      "Predicción (3 clases): negative\n",
      "Predicción (5 clases): 3\n",
      "\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Frase: This app works perfectly.\n",
      "Predicción (3 clases): positive\n",
      "Predicción (5 clases): 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lista de frases para analizar\n",
    "texts = [\n",
    "    \"This app is amazing, I use it every day!\",\n",
    "    \"Terrible experience, the app crashes all the time.\",\n",
    "    \"It's a decent app, but it has some bugs.\",\n",
    "    \"The app is okay, but there are better alternatives.\",\n",
    "    \"I hate the new update, it ruined everything.\",\n",
    "    \"This app works perfectly.\"\n",
    "]\n",
    "\n",
    "# Analizar cada frase\n",
    "for text in texts:\n",
    "    analyze_sentence(text, model_3_classes, model_5_classes, class_names_3, class_names_5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
